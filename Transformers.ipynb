{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODdDMslM66S58BP2pshoAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NourSoltani/ML-Learn/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sequential data**"
      ],
      "metadata": {
        "id": "OpGvg5O1tvpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part one: Learning from sequences\n",
        "# Part two: RNNs\n",
        "# Part three: LSTMs\n",
        "# Part four: CNNs for sequential data\n",
        "# Part five: ELMo, a case study"
      ],
      "metadata": {
        "id": "_3xn6nlmttM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part one: Learning from sequences**"
      ],
      "metadata": {
        "id": "N1bIvZm-uL8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#So: what kind of sequential data can we expect? The simplest case is probably numeric one-dimensional sequential data: a timeseries.. we can also have numeric n-dimensional data, or a dataset can be symbolic; \n",
        "#where at every time step, we are given a symbol from a fixed vocabulary; The prime example of this is probably language, which can be viewed as a symbolic sequence in two ways; we can break it up into\n",
        "#words in which case we have a very large vocabulary, and at each time step we are given one word, or we can view langugage as a sequence of characters which gives us a much smaller vocabulary and a much longer\n",
        "#sequence. and datasets generally come in one of two types; either single sequence or set of sequences\n",
        "#One of the interesting method in order to split your data between train and test is to use a method called:walk-forward validation.\n",
        "#So sequences: consisting of numbers, vectors or symbols.\n",
        "#and Dataset: consisting of a sequence per insatnce, or a sequence of instances\n",
        "#sequence models: operate on inputs of different lengths (using the same weights).\n",
        "#input: raw sequence data\n",
        "#output: classification, regression, token prediction, sequence-to-sequence.\n",
        "#layers: sequence-to-sequence. But what's a sequence-to-sequence layer? It's a layer that takes as input a sequence of vectors of length t and produces as output another sequence of vectors again of length t,and\n",
        "#the input and output dimensions may be different, but the length of the sequence is the same in both cases; We can generalize this if we want to from vectors to tensors, but practically for this lecture, we will\n",
        "#stich to vectors. And again, the defining property of a sequence layer is that the same layer with the same weights can be applied to sequences of different lengths.\n",
        "\n",
        "#IF YOU WANT TO THINK OF A CONCRETE EXAMPLE, JUST TAKE A CONVOLUTION; JUST THINK OF A ONE-DIMENSIONAL CONVOLUTION IN PLACE OF SEQUENCE TO SEQUENCE LAYER. \n",
        "#The first important proerty that sequence to sequence layers may or may not have is causality; A causal layer is a layer that can only look backward in the sequence.\n",
        "#First if our data has discrete inputs, then we need to turn that into a sequence of vectors; One way to do that is by what's called one-hot vectors(with same length as the number of options).\n",
        "#Another approach is to create embedding vectors; and here the idea is that for every element in our set of options; every token in our vocabulary, we create a vector of parameters that represents that object.\n",
        "#and theses embedding vectors contain parameters; so all of the numbers iof input elements will be learned during the training process of the NN.\n",
        "#Embedding vectors are not specific just to sequence learning; we'll see them in some other settings as well. "
      ],
      "metadata": {
        "id": "r90G01nIuAC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model configurations: \n",
        "# Sequence-to-sequence: POS tagging, machine translation, robot control, generation\n",
        "# Sequence-to-label: Classification, regression\n",
        "# Label-to-sequence: generative models\n",
        "# Label+seq-to-sequence: Teacher forcing\n",
        "###To recap: Sequence-to-sequence models are defined by a set of fixed weights that can be applied to variable length inputs/ Three instances(RNN,CNN,Self-attention)/ Embeddings, padding, masking, and packing\n",
        "# can help us to pre-process our data and to feed it to a DL model, and we've seen how versatile these seq-to-seq models can be: because we can train seq-to-seq, label-to-seq, seq-to-label, autoregressive training\n",
        "#,teacher forcing, and more."
      ],
      "metadata": {
        "id": "_NROBFlQTljP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###RNNs: RNN is basically a name for any NN that has a cycle in it.\n",
        "###How to train RNNs? Unrolling. \n",
        "###Note of the following:\n",
        "#     -RNNs are sequence-to-sequence layers(shared weights, variable length)\n",
        "#     -RNNs are causal: Only backwards connections\n",
        "#     -Potentially unbounded memory(theoritically: vanishing/exploding gradient): If we unroll over the whole sequence, there's a computational connection between the first element of the input sequence, and the last element of the output sequence, no\n",
        "#      matter how long the sequence is. And this is where they differ from CNNs for instance; because we know CNNs have a finite receptive field; so any of these outputs in the CNN can only depend if we have\n",
        "#      one convolutional layer with a size three kernel, then any output of this CNN can only depend on three of the inputs, and it cannot look infinitely far along the input sequence, in contrast to the RNN\n",
        "#      which can always look infinitely far back in the input sequence. The drawback, or the price we pay for this potentially long memory is that RNNs are quite slow to evaluate, because they need to be processed\n",
        "#      sequentially; what that means is that in order to evaluate the fourth hidden layer for instance, we first need to evaluate the hidden layer h3, and in order to evaluate this latter, we need to evaluate h2.. And so on.\n",
        "#      ==> We cannot evaluate these four layers in parallel, in contrast to CNN, where if we look to the four outputs, each output can be given to a thread which in parallel computes that particular output, \n",
        "#     based on the weights of the convolution and the inputs; they don't need to refer to each other in order to know their own value, and this makes RNNs a little bit slower than most other neural network layers.\n",
        "#     (We are talkin here about just one layer: RNN layers(which contain 4 hidden layers) and Conv1D layer(which also has a fully connected layer 4 input and 4 outputs))\n",
        "###We can solve the problem of vanishing gradient by replacing the sigmoids by ReLUs, and by making sure the weight matrices are properly initialized, and perhaps even by adding the occasional normalization step\n",
        "# in between, but in the late 90s when these recurrent neural networks were very popular, those options weren't available yet, and instead people came up with a very different solution, which is known as LSTM.\n"
      ],
      "metadata": {
        "id": "bsaOJaTfi9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####CNNs for sequential data\n",
        "https://www.youtube.com/watch?v=rT77lBfAZm4&list=PLIXJ-Sacf8u7756f8QFM_FNZQxdJov8f4&index=4&ab_channel=DLVU\n"
      ],
      "metadata": {
        "id": "zilFNCrbpiFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJCsJ8f5PTRL"
      },
      "outputs": [],
      "source": [
        "#Part one: Self-attention: The basic sequence to sequence operation that drives all transformer models.\n",
        "#Part two: Transformers: We will look how to build up this self-attention into a complete transformer model\n",
        "#Part three: Famous transformers: We will look at some famous examples of transformer models, and we'll look at the finer details of how they're constructed and how they were trained.\n",
        "#Part four: Advanced tricks: that are being studied to improve the performance of transformer models in various ways."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART ONE: SELF-ATTENTION**"
      ],
      "metadata": {
        "id": "-qgNQuxoUwGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Before this, we have talked for the first time about sequence-to-sequence layers; and this are neural network layers that take as input a sequence of tensors; usally a sequence of vectors, and produce a sequence\n",
        "#sequence of vectors as an output as well where the both of the sequences have the same length, and the direction in which the sequence extends is called \"the time direction\".\n",
        "##RECAP:\n",
        "#Defining property: Can handle sequences of different lengths with the same parameters.\n",
        "#Versatile: label-to-sequence, sequence-to-label, sequence-to-sequence, autoregressive training.\n",
        "#Causal or non-causal: causal models can only look backward.\n",
        "\n",
        "#The aim of self-attention as a sequence to sequence layer is to give us the best of both worlds; parallel computation(like cnn) and long dependencies(like RNN: The ability to look at any point of the sequence\n",
        "#before or after the current output)\n",
        "#There's Simple self-attention: the basic idea\n",
        "#Practical self-attention: Adding some bells ane whistles"
      ],
      "metadata": {
        "id": "8_nr-2BnUz1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#At heart, self-attention is a very simple operation; We have a self-attention layer with a sequence of input vectors, and a sequence of output vectors, and the basic operation that produces any given output vector\n",
        "#is simply a weighted sum over the input vectors: yi=ΣWij Xj : For every output we've a set of six weights in this case since we've 6 inputs: We simply perform a weighted sum over the input vectors, and that's\n",
        "#our output vector, and we do this for every output, and that gives us our sequence of outputs.\n",
        "#Now the trick that makes this special is that this Wij is not a parameter of the model, but it's a derived value that we compute from the input.\n",
        "\n",
        "#To compute Wij, we calculate w'ij=transpose(Xi).Xj, and then we apply a softmax operation: Wij=exp(W'ij)/Σ(exp(W'ij))\n",
        "#Of course if we do this naively, it would involves a lot of loops and we don't like loops in deep learning, we want to vectorize our operations. Fortunately, this operation is a very easy one to vectorize,\n",
        "# we do that as follows: To compute all the raw weights of all inputs and all output positions i and j, we can simply compute a large matrix of all dot products of x with itself: So every this matrix W'(W'=transpose(X).X)\n",
        "# contains every dot product of every input vector with every other input vector to apply the softmax(W=softmax(W')), we simply apply it to the matrix raw wise, so that the elements of all raws are positive\n",
        "#and sum to 1. And then we simply multiply this weight matrix with our input matrix X to give us the matrix Y which contains all the weighted sums computed in one matrix multiplication.\n",
        "\n",
        "###To properly understand what's happening here, I'd likd to point out a few things that we may not immediately have noticed.\n",
        "#The first is that in this particular version of self-attention(simple self-attention), the weight from an input vector at one position to the same position so the weight ii from Xi to Yi is usually the biggest,\n",
        "#because this weight is defined by the dot product of a vector with itself, and that's usually a higher value that the dot product of a vector with some other vectors; Well, this is not a big problem, but\n",
        "#we'll allow this to change later; It just means that what this simple self-attention is doing is essentially keeping every input vector the same, but mixing in a little bit of the values of the other input\n",
        "#vectors according to this weight, and we'll add a few simple mechanisms later that will allow us to change this behavior if necessary.\n",
        "\n",
        "#Note also that a simple self-attention like this has no parameters: There's nothing we can do; No numbers we can set to change the behavior of the sequence-to-sequence layer; Its behavior is entirely driven\n",
        "#by whatever mechanism generates these input vectors: So for instance, if we take one embedding layer and stick one simple self-attention layer on top of it, then the embedding layer entirely drives the behavior\n",
        "#of the model.\n",
        "#Thirdly, and this is probably one of the bigger reasons why self-attention works so well, note that it is fundamentally a linear operation; The whole of self-attention is one matrix multiplication of w by x\n",
        "#resulting in y, and of course w is derived from the values of x: There's a linear operation between X and Y. non-vanishing gradients are through Y=WX.transpose(X), vanishing gradients through W=softmax(transpose(X)X)\n",
        "# which means here we get a non-linearity at the price of potentially vanishing gradients; So in this way, we get the best of both worlds seperated into two parts of the computation graph(look page 7-middle): \n",
        "#linear operation with non-vanishing gradients, and a non-linear operation with vanishing gradients.\n",
        "\n",
        "#Another bonus: Note that self-attention has no problem looking far back into the sequence. In fact, if we contrast it with the recurrent neural network, then we see that in the recurrent neural network, \n",
        "#the further back we go into the sequence, the more computation steps there are between an input vector and an output vector. That's not the case for self-attention: At every point in the sequence, there are \n",
        "#as many steps between that input point and any of the output points as at any other point in the sequence; This is because at heart, self-attention is really more of a set-model than a sequence model. As we've\n",
        "#set it up this simple half attention, the model has no access to the sequential structure of the input, and we'll fix it later by encoding the sequential structure into the embeddings, but that's something\n",
        "#for the next part. For now, we'll just look at this self-attention operation as a set; as more of a set-to-set layer, then a sequence-to-sequence layer.\n",
        "#And another way of saying this is that self-attention is permutation equivariant; that's if we permute or shuffle the input sequence, it then it makes no difference whether we first permute and then apply the\n",
        "#self-attention or first apply self-attention and then permute, we get the same result. So, that's the basic operation of self-attention. \n",
        "\n",
        "#Now before we move on, it pays to build a little bit of intuition for why this works so well. And one of the big reasons why it works is the power of the dot product.\n",
        "#We have the example of users and movies, and the act \"likes\" between them, and our job is to predict which other movies these users might like; One thing we might do is to collect feauture vectors for each\n",
        "#of these users and for each of these movies,\n",
        "#FOr instance; user u ____________\n",
        "#                     |@@|[][]|//|    @@: likes thriller [][]: likes action, and //: likes comedy. \n",
        "#                     ------------  \n",
        "#And the same for movie m: ##: Thriller    {}{}: Action, and \\\\:comedy \n",
        "#And if we collect  the feature vectors like this, then we can simply take the dot product to get a good prediction for how much the user will like the movie: score=u1*m1+u2*m2+u3*m3 (we are getting these terms\n",
        "#that are just the feature of each user multiplied by the corresponding feature in the movie), and the first thing to notice here is that the dot product very intuitively takes into account the signs of the feautes\n",
        "#For instance, if the user likes thriller, and a movie has thriller, then these 2 values will multiply and increase the score, but if the user don't like thriller, and their feature vector has a negative value\n",
        "#at theat position, and a movie doesn't contain thriller, or is un-thriller to the extent that it also has a negative value at that position of the feature vector, then the two minuses will cancel out, and\n",
        "#the score will also uncrease..\n",
        "#Secondly, the magnitudes of the values in the feature vectors behave very naturally; If a user is fairly ambivalent to thriller, then that part of the feature vector will be close to zero, and so on.\n"
      ],
      "metadata": {
        "id": "HQp2llJcEJ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###AND IF WE DON'T HAVE FATURE VECTORS LIKE THIS, OR WE DON'T FEEL LIKE COLLECTING THEM, THEN WE CAN JUST LEARN EMBEDDING VECTORS INSTEAD OF FEATURE VECTORS, AND THAT'S THE BASIC MECHANISM BEHIND A LOT OF\n",
        "#RECOMMENDER SYSTEMS.\n",
        "#So how does this mechanism work in a self-attention model? (look page8-top): We have inputs --> Embedding layer -->Embeddings -->Simple self-attention --> output sequence --> Global max pooling\n",
        "#This was a simple classification model with a simple self-attention layer. Here we have a model with 2 layers; One embedding layer that transforsm the input words to input vectors, and one simple self-attention\n",
        "#layer which leads to an output sequence, and the vectors in the output sequence are summed together to give us a single vector from which we perform the classification.\n",
        "#Now if we did this without the self-attention layer, we would essentially have a model where each word can only contribute to the output score independetly of every other word; This is known as a bag of words\n",
        "#model: For instance, in this case,if we have as input: \"The restaurant was not too terrible\", the word \"terrible\" would probably cause us to predict that this review is negative. In orderto see that it might\n",
        "#actually be a positive review, we need to recognize that the meaning of the word \"terrible\" is modderated, and in fact inverted by the presence of the word \"not\" and this is what self-attention can do for us.\n",
        "#So in this case, what we would hope that the model would learn is that the words \"not\" and \"terrible\" can interact in an important way for this task; So we would hope to learn that the embedding vector for \n",
        "#the word \"not\" is learned in such a way that it has a load dot product with the embedding factor for the word \"terrible\", So that if the two occur together in a sentence, we can lower the probability of terrible\n",
        "#having a meaning  that contributes negatively to the score, because there's a possibility that the word \"not\" occur in a way that inverts the meaning of the word terrible; we can't be sure of course with one \n",
        "#self-layer but with the features we will add later, and with a larger stack of self-attention layers, that problem can be solved as well"
      ],
      "metadata": {
        "id": "FaIKyUqa8OB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There's extra features that we can add to self-attention to make it a little bit flexible and a little bit more powerful, and we will look to three different features: \n",
        "###1)Scaled dot product\n",
        "###2)key, value, and query transformations\n",
        "###3) multi-head attention: which essentially boils down to applying multiple self-attentions in parallel.\n",
        "\n",
        "####First; Scaled self-attention: The problem we're trying to solve here is that as the dimensionality of the input vectors grows so does the average size of the dot product. and that growth is by a factor of\n",
        "#the square root of k.\n",
        "# W'ij=[transpose(Xi)*Xj]/ √k #where k is the input dimension. # So if we divide by the square root of k, then we normalize the average dot product; which keeps the weights within a certain range where we \n",
        "#don't suffer from vanishing gradients on the softmax operation: So this is a very simple trick that can really helpl learning.\n",
        "\n",
        "####For the second feature, we need to recognize that every vector in a self-attention operation occurs in 3 different positions; First as a vector that is used in the weighted sum that ultimately provides the\n",
        "#output: we call that the value. Second, as the input vector that corresponds to the current output matched against every other input vector; This is called the query. And third, the third that the query is\n",
        "#matched against,which is called the key. And these names derive from a way of thinking about this mechanism as a kind a soft version of a dictionary; where the key, the query and the value are all vectors\n",
        "#of the same size, and instead of having a query that matches only one key, every key matches the query to some extent as determined by their dot product, and instead of returning a single value, that of the\n",
        "#key that matches the query, we return a mixture of all values with softmax normalized dot products as the mixture weights. And in this way of looking at our mechanism, self-attention is jst an attention mechanism\n",
        "#with keys, queries, and values all coming from the same set, and that's where the name self-attention comes from.\n",
        "#So, to make self-attention a little bit more powerful, we can introduce some transformations for these three different roles; So that even though we are using the same vector in all three roles, they can behave\n",
        "#differently depending on what role they're taking on. And we do this simply by linear transformations.\n",
        "# so for if for every role we introduce a weight matrix and an associated bias and we compute a key vector by passing the input vector through the key transformation the query vector by passing the input\n",
        "#vector through the query transformation and the same for the value.\n",
        "#ki= Kxi+bk\n",
        "#qi=Qxi+bq\n",
        "#vi=Vxi+bv\n",
        "\n",
        "#so this makes the self-attention operation a little bit more flexible in what it can do and note also that because we've now introduced this transformation the self-attention operation has\n",
        "#some parameters so the operation itself now also has has some numbers that we can change to influence the behavior of the layer"
      ],
      "metadata": {
        "id": "9avfNqiBIcnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###TWOHEAD SELFATTENTION\n",
        "#the final feature we will add is what's called multi-head attention and the necessity for multi-head attention derives from the idea that different words relate to each other by different relations\n",
        "#so for instance, the word terrible in this sentence relates to the word relates to the words not and to in that the words not and to moderate and invert the meaning of the word terrible\n",
        "#so the presence of the words not and the presence of the word to change the meaning of the word terrible but the relation between the word restaurant and terrible is completely different\n",
        "#the word terrible describes the property of the restaurant. Now in order to allow the network to model all these different kinds of relation in one self-attention operation we split the self-attention\n",
        "#into different heads which are basically self-attention layers applied in parallel and in practice that looks like this we start with an input sequence we pass the input sequence through some linear operations \n",
        "#to decrease its dimensionality so here we have a two head itself attention so we pass the input to two projections down to a lower dimensionality w1 and w2 each is fed to a separate self attention\n",
        "#so we have self tension 1 and self tension 2 each with their own key query and value transforms we get two sequence vectors out which we concatenate and pass through a final output transformation to give us \n",
        "#the output sequence"
      ],
      "metadata": {
        "id": "iG5-R0hW94sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###MULTIHEAD SELFATTENTION\n",
        "\n",
        "# now note that there are two different ways of implementing this as we've drawn it in the previous slide we first multiply each input vector x by this gray matrix here which turns it\n",
        "# into a vector that will be split into two; one input for each head and then the input for each head will be multiplied by another matrix to produce its key by another matrix to produce its\n",
        "# query and by another matrix to produce its value but since these two operations are applied in sequence and they're both linear operations we can also multiply them together to produce one\n",
        "# equivalent linear operation and if we do that for each one of these three matrices on the right ones for the key one's for the query and once for the value what we get is three different matrices\n",
        "# that immediately produce the key the query and the value to go to the different heads and what this shows us is that the multi-headed self-attention if we apply it in this way and we ignore the w o transformation\n",
        "# that is applied after concatenation the number of parameters for the single head self-attention and for the multi-self attention are the same so in this sense we are not adding a lot more parameters by\n",
        "# splitting the self-attention up into separate heads and that's the last feature we wanted to add."
      ],
      "metadata": {
        "id": "x_ztVOZLJ4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to recap we've introduced self-attention which is a sequence to sequence layer that allows for parallel computation and unbounded long-term memory it's fundamentally a set to set layer it has no access to\n",
        "# the sequential structure of the input this is something that we need to solve later on and a large part of the behavior comes from the parameters upstream.\n",
        "# Now the real power of the  self-attention comes primarily fr 'om its simplicity and its cheapness to compute this means that we can stack a lot of self-attention operations together\n",
        "# and build very large and deep models with them and these are called transformer models and that's what we're going to be discussing in the next video\n"
      ],
      "metadata": {
        "id": "kSPh48vEL5NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers**"
      ],
      "metadata": {
        "id": "khkpFqx_z_d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To get from a self-attention layer to a full-fledged model, we need to repeat it a number of times in a controlled fashion; If we do that, we get what's called a transformer model, and that's what we are going\n",
        "#to talk about in this video.\n",
        "##Transformer: Any sequence-based model that primarily uses self-attention to propagate information along the time dimension; So, we can add some other features and some other types of layers, but the main layer\n",
        "#that'S responsible for propagating along the time dimension will be the self-attention.\n",
        "#We'll limit ourselves to sequence models in this lecture, but actually there are now transformers in other domains as well. For instance, there are image transformers, and graph transformer.\n",
        "#And the basic idea there is that our input consists of a set of basic units; in the case of images it's pixels, and in the case of graphs; graph nodoes that are connected by some structure: In images: the pixel\n",
        "#grid, and in graph nodes, the topology of the graph.\n",
        "###And the idea of any tranformer is that it's a model that primarily uses self-attention to propagate informations between these basic units of our innstances along the structure that we are given, along the pixel\n",
        "#grid or along the graph. But as we have said, in this lecture, we'll limit ourselves to sequence models.\n",
        "\n",
        "\n",
        "##More broadly: Any model that primarily uses self-attention to propagate information between the basic units of our instances.\n",
        "#pixels -> Image transformer\n",
        "#graph nodes ->Graph transformer.\n",
        "\n",
        "\n",
        "#The main strategy that people tend to use to build transformers is to define a transformer block, which is a set of operations that are wired together in a certain way, and to then repeat that transformer block\n",
        "#a number of times. So, the exact architecture of transformer block differs from model to model, but in most cases it looks something like this:\n",
        "class Block(nn.Module):\n",
        "  def forward(self,x):\n",
        "    y=self.layernorm(y) #The input sequence is fed through a normalization known as layer normalization\n",
        "    y=self.attention(x) #self-attention\n",
        "    x=x+y\n",
        "\n",
        "    y=self.layernorm(x) #layer normalization\n",
        "    y=self.linear(x) #feed-forward\n",
        "    return x+y\n",
        "#The feed-forward layer here operates with the same parameters on every token of the input sequence in isolation; This means that, as we said before, the only operation that propagates information along the \n",
        "#time dimension is the self-attention, and the other 3 operations operate on every input token in isolation.\n",
        "#An example of a sequence to label transformer is: Input embeddings ->Transformer block -> Transformer block -> Transformer block -> Output sequence -> global sum/avg/max pooling.\n",
        "#A problem that we need to deal with is the lack of sequential structure in the self-attention, since obviously the meaning of a sentence often depends on the exact ordering of the words, and if we feed it\n",
        "#through a simple classification transformer to do for instance \"sentiment classification\", what we see is that the output vectors will be the same for these two sentences: This resturant is not a real restaurant,\n",
        "#it's a filthy burger joint and This is not a filthy burger joint, it's a real restaurant.\n",
        "#So, we need to break this equivariance, and we need to tell the transformer about the structure of the input sequence; We do so by communicating the position of the input tokens, and we'll look at three\n",
        "#ways of doing this: position embedding/position encoding/ and relative positions\n",
        "\n",
        "#####The simplest of these are positional embeddings: In positional embeddings just like we assigned an embedding vector to every word in our vocabulary, we also assign an embedding vector to every position in our\n",
        "#sequence from one to however long we expect our sequences to be, and then we can just sum these two together for every word in our input sequence: For instance, we have the sequence \"the man pets the cat again\",\n",
        "#the word \"the\" occurs twice, but they result in different input vectors, because in the first case, it's summed with the position embedding for the position 1, and in the second occurence, it's summed with \n",
        "#the position embedding for the position 4. ##AND WE CAN DO THIS AT EVERY BLOCK, OR WE CAN DO IT JUST ONCE.\n",
        "#This is very easy to implement, but the drawback is that we're basically giving our transformer model a fixed maximum length: So if we encounter after training a sequence that's longer than the longest sequence \n",
        "#that we encounter during training, then the position embeddings for the end of that sequence will not be trained, and there we cannot expect good peformance on such sequences.\n",
        "\n",
        "####An approach that generalizes a little better at least in theory is that of position encodings. Here we take the same principle; We represent the positions in our sequence by vectors, but here they are not\n",
        "#embedding vectors, they are not learned, they are simply fixed constants that we defined beforehand, and the trick here is to define these vectors by a series of functions one per dimension that follow a\n",
        "#predictabl pattern. So theoritacally and ideally, a transform would generalize a little bit better to longer sequences if poistion encodings are used. The drawback it that it's a little bit more difficult to \n",
        "#implement, and they are a bit more ad-hoc choices to make in exactly which position encodings you use"
      ],
      "metadata": {
        "id": "uoN9-4AD0CMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So, to restate, we have position embeddings which are easy to implement, flexible, but don't offer any generalization between the observed sequence length during training.\n",
        "#Position encondings: which are slightly harder, but offer the possibility at least of a little bit more generalization.\n",
        "#And then, relative positions which we can use with both embeddings and encodnings, but they must be implemented by adapting the self-attention itself, so they're not as easy as just summing a bunch of vectors\n",
        "#to our input vectors"
      ],
      "metadata": {
        "id": "9bDt29GCqRZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pytorch Transformers from Scratch (Attention is all you need)**"
      ],
      "metadata": {
        "id": "YFBRyLjs0PT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have an encoder and decoder; The encoder is constituted from a transformer; which is constituted from: Multi-Head Attention->Add & Norm -> Feed forward -> Add and Norm ;where the Multi-head attention is basically constituted from self-attention block\n",
        "# The decoder itself is constitued from a transformer also, a masked multi-head attention and \"Add & Norm\" before it.\n",
        "# The transformer network is permutationaly invariant.. that's why we add the \"Positional encoding\".\n",
        "# The thing about transformers that made them so great is the fact that all operations are able to be done in parallel,  which is to contrast to sequence models like RNN or LSTM.. But this really big strength has one problem\n",
        "# which is if we look at translation where we have a target translated text. This translated text is all sent into decoder at the same time. So, let's just say that the first element is a <start> token, and then the next element is\n",
        "# the first translated word, and then the first output that we have worn(i think it's norm) from the decoder just corrensponds to the second element which we send in the targe sentence: So, if we allow the encoder to have all this information,\n",
        "# this is going to be super easy simple; It will just learn to use the provided target translation, and it will just learn a simple mapping, and really not learn anything about translating text. So, what we do is that we mask the\n",
        "# target input to the decoder; So that the first output of the decoder only had access to the first element and then the second output only had access to the first and second input to the decoder.\n"
      ],
      "metadata": {
        "id": "O-3wLjMlkFGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Attention Mechanism\n",
        "#We inherit from nn.Module\n",
        "class SelfAttention(nn.Module):\n",
        "  #We've an embedding and we're going to split this embedding into different parts; for example divide the sequence into eight parts; and how many parts we're going to split gonna be called \"heads\".\n",
        "  #So if we've, for instance, embeded size equal to 256, and we've heads equal to 8, then we're gonna split it into 8 by 32 parts.\n",
        "  def __init__(self,embed_size, heads):\n",
        "    # The attributions that we have in our class are: embed_size/ heads/ head_dim/  values/ keys/ queries/fc_out: These last four are layers.\n",
        "    #And the inputs are: embed_size and heads.\n",
        "    super(SelfAttention,sel).__init__()\n",
        "    self.embed_size=embed_size\n",
        "    self.heads=heads\n",
        "    self.head_dim=embed_size // heads\n",
        "    assert(self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
        "    # The assert keyword is used when debugging code. The assert keyword lets you test if a condition in your code returns True, if not, the program will raise an AssertionError (In our case: return \"Embed size need to...\")\n",
        "\n",
        "\n",
        "\n",
        "    # We're going to define the linear layers that we're gonna send our values, keys, and queries through.\n",
        "    self.values = nn.Linear (self.head_dim,self.head_dim,bias=False) #The first and second elements are for the size of each input sample and size of each outptut sample respectively, and if bias=False, then the layer will not learn an additive bias. Default: True\n",
        "    self.keys = nn.Linear (self.head_dim,self.head_dim,bias=False)\n",
        "    self.queries = nn.Linear (self.head_dim,self.head_dim,bias=False)\n",
        "    # Then after we concatenate; We're gonna do fully connected out:\n",
        "    self.fc_out= nn.Linear(heads*self.head_dim, embed_size)  #heads*self.head_dim is equal to embed_size\n",
        "\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    # The first thing is that we are going to get the number of training examples\n",
        "    N=query.shape[0] #that's gonna be how many examples we send in at the same time\n",
        "    value_len, key_len, query_len =values.shape[1], keys.shape[1], query.shape[1]\n",
        "    # These lengths are going to depending on where we use the attenion mechanism is going to be corresponding to the source sentence length, and the target sentence length, but since we don't know \n",
        "    # exactly where the mechanism is used; either in the encoder, or which part in the decoder: those are going to vary, so we just use the abstract of saying we just use it abstractly and say value length, key length, and query \n",
        "    # length, but really they will always correspond to the source sentence length and the target sentence length.\n",
        "\n",
        "\n",
        "    #Split embedding into self.heads pieces\n",
        "    values=values.reshape(N, value_len, self.heads, self.head_dim) ###The last two; \"self.heads\" and \"self.head_dim\" is where we're spliting it since this was before a single dimension of just embed size: now it's going to be \n",
        "    #self.heads, and then self.head_dim. \n",
        "    #And we are going to do the same thing for the keys.\n",
        "    keys=keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries=query.reshape(N,key_len, self.heads, self.head_dim)\n",
        "    #And then what we gonna do is we want to multiply the queries with the keys, and the output from that is gonna  be called \"energy\".\n",
        "\n",
        "    energy=torch.einsum(\"nqhd,nkhd->nhqk\",[queries,keys]) #We are going to use it for matrix multiplication where we have several other dimensions, so let's bring out the shapes first:\n",
        "    #where n for batch size,q for query length, h for the heads, k for key lengthand then d for the head dimension. \n",
        "    #After th -> is the output shape \n",
        "\n",
        "\n",
        "    #queries shape: (N,query_len, heads, heads_dim)\n",
        "    #keys shape: (N,key_len, heads, heads_dim)\n",
        "    #energy shape: (N,heads, query_len, key_len) \n",
        "    ######Let's say that the query_len is the target source, and the key length is the source sentence, then it's kind of\n",
        "    #saying that okay for each word in our target(qurey), how much should we pay attention to each word in our input; in the\n",
        "    #store sentence(key).\n",
        "\n",
        "    if mask is not None:\n",
        "      #The above line means that we are sending a mask.\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\") ) #If the element of the mask=0, then that means that we want\n",
        "      #to shut that off; So that it doesn't impact any other.\n",
        "      #So essentially, as we saw previously, the mask for the target is gonna be a triangular matrix(used in the masked multi-head attention )\n",
        "      #But anyways, the element when we're gonna close it is zero, and what it means to close it is that we're gonna replace\n",
        "      #those elements with a float; where we're gonna set it to essentially -infinity, but just for numerical, it doersn't bring that\n",
        "      #That's why we set it to a very very small value.\n",
        "    \n",
        "\n",
        "    #We're going to run this through softmax where the equation is: Attention(Q,K,V)=Softmax{(Q.transpose(K))/√dk}.V\n",
        "    attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "    ####This means that we're gonna do dimension equals=3 and this means that we're normalizing across the key length,\n",
        "    #which for example would be depending on (As we have been saying) where we use the attention mechanism; It's going, let's\n",
        "    #say the \"key_len\" is the source sentence, \"query_len\" is the target sentence length, then that would say: how much we want\n",
        "    #to(essentially we're making the attention scores normalized to one across te source sentence): So, if the first for example\n",
        "    #is 0.8, then that means that we're paying 80% attention to the first word in the source sentence.\n",
        "    \n",
        "\n",
        "    #Now we want to multiply the attention with the values, so we are going to do that with our famous einsum.\n",
        "    #Down;: l=the dimension that we want to multiply across== the key length and the value length match now, so both are l.\n",
        "    #And also we wan to do the concatenation part, so we can do that instantly after this torch.einsum by reshaping; so\n",
        "    #we are just concatenating those.\n",
        "    out = torch.einsum(\"nhql,nlhd ->nqhd\",[attention,values]).reshape( \n",
        "        N,query_len, self.heads*self.head_dim   \n",
        "    )\n",
        "    \n",
        "    # attention shape: (N, heads, query_len, key_len)\n",
        "    # values shape: (N, value_len, heads, heads_dim)\n",
        "    # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions \n",
        "    \n",
        "    out=self.fc_out(out) #What we want to do lastly is hust send it through fc_out; and this fc_out won't change the dimension\n",
        "    #since the fc_out just maps the embed_size to embed_size\n",
        "    return out"
      ],
      "metadata": {
        "id": "S7_Imjva0RE5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So now, as we have the attenion, this is gonna be a lot easier for us: We're just gonna create  the TransformerBlock, \n",
        "#The architecture is as follows: Multi-Head Attention -> Add & Norm -> Feed Forward -> Add & Norm.\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.attenion = SelfAttention(embed_size,heads) #This represents the Multi-Head Attention\n",
        "    \n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size) #Well layerNorm and batchNorm are very similar except that batchNorm takes the average across the batch, and then \n",
        "    #normalizes, whereas, layerNorm just takes an average for every single example: So think this layernorm has more computation\n",
        "    #than batchnorm\n",
        "    \n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,value, key,query,mask):\n",
        "    attention=self.attention(value, key, query, mask)\n",
        "     \n",
        "    x=self.dropout(self.norm1(attention+query)) #Wy did we write \"attention+query\"? for the skip connection.\n",
        "    forward=self.feed_forward(x)\n",
        "    out= self.dropout(self.norm2(forward+x))\n",
        "    return out\n",
        "    \n",
        "#And now, we're goona try to stick this together and form both; encoder and decoder."
      ],
      "metadata": {
        "id": "6csCXrOh0_Vg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ARCHITECUTRE OF ENCODER: Inputs ->(+) Postional Encoding -> TransformerBlock * Nx..\n",
        "#We've to set the hyperparameters of the model under def __init__\n",
        "#Why src_vocab_size? Because now we're going to do the embedding and all of those things as well.\n",
        "\n",
        "#max_length is related to the positional embedding: Positional embedding is depending on position.. So, we need to send in\n",
        "#how long is the max sentence length.\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      device,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      max_length):\n",
        "    super(Encoder,self)._init__()\n",
        "    \n",
        "    #embed_size, device, word_embedding, position_embedding, layers, and dropout are the attributes and methods of Encoder:\n",
        "    self.embed_size=embed_size\n",
        "    self.device=device\n",
        "    self.word_embedding  = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "    \n",
        "    #And now in the following, we're going to use it in order to map several different modules together(using nn.ModuleList):\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            TransformerBlock(\n",
        "                embed_size,\n",
        "                heads,\n",
        "                dropout=dropout,\n",
        "                forward_expansion=forward_expansion,\n",
        "            ) \n",
        "\n",
        "        ]\n",
        "    )\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "  \n",
        "\n",
        "  #Now, we're ready to do the forward part: we're gonna send in just one input to the forward, and we're going also to\n",
        "  #send in a mask.\n",
        "  def forward(self, x,mask):\n",
        "    N,seq_length= x.shape\n",
        "    positions = torch.arange(0,seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.word_embedding(x)+ self.position_embedding(positions))\n",
        "    #The only thing that makes it aware of the positions is \"position.embedding\" (where we are going to send the positions)\n",
        "    \n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "YYlEjKX-bVmG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size,heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block=TransformerBlock(\n",
        "        embed_size, heads, dropout, forward_expansion\n",
        "    )\n",
        "    self.dropout =nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x, value, key, src_mask, trg_mask):\n",
        "    attention = self.attention(x,x,x,trg_mask)\n",
        "    query=self.dropout(self.norm(attention+x))\n",
        "    out = self.transformer_block(value, key, query, src_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "oGzZY6k9rVh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        device,\n",
        "        max_length,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "5lj2f8WUs7sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        embed_size=512,\n",
        "        num_layers=6,\n",
        "        forward_expansion=4,\n",
        "        heads=8,\n",
        "        dropout=0,\n",
        "        device=\"cpu\",\n",
        "        max_length=100,\n",
        "    ):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            device,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_size,\n",
        "            num_layers,\n",
        "            heads,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "            device,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # (N, 1, 1, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "RB4rV6Wys8GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
        "        device\n",
        "    )\n",
        "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "    src_pad_idx = 0\n",
        "    trg_pad_idx = 0\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_size = 10\n",
        "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
        "        device\n",
        "    )\n",
        "    out = model(x, trg[:, :-1])\n",
        "    print(out.shape)"
      ],
      "metadata": {
        "id": "w6zRBKnHs-QL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}